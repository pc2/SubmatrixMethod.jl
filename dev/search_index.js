var documenterSearchIndex = {"docs":
[{"location":"devel/analysis/#Analysis","page":"Analysis","title":"Analysis","text":"","category":"section"},{"location":"devel/analysis/#Density","page":"Analysis","title":"Density","text":"","category":"section"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"<img src=\"../../assets/imgs/time_vs_density_1000.png\" width=\"80%\"/>\n<br>","category":"page"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"import Dates\nstring(\"Last run: \", Dates.unix2datetime(mtime(\"../../../analysis/scaling_density/time_vs_density_1000.png\")))","category":"page"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"<br>\n<img src=\"../../assets/imgs/time_vs_density_10000.png\" width=\"80%\"/>\n<br>","category":"page"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"import Dates\nstring(\"Last run: \", Dates.unix2datetime(mtime(\"../../../analysis/scaling_density/time_vs_density_10000.png\")))","category":"page"},{"location":"devel/analysis/#Number-of-Threads","page":"Analysis","title":"Number of Threads","text":"","category":"section"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"<img src=\"../../assets/imgs/time_vs_nthreads_0.001.png\" width=\"80%\"/>\n<br>","category":"page"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"import Dates\nstring(\"Last run: \", Dates.unix2datetime(mtime(\"../../../analysis/scaling_multithreading/time_vs_nthreads_0.001.png\")))","category":"page"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"<br>\n<img src=\"../../assets/imgs/time_vs_nthreads_0.0001.png\" width=\"80%\"/>\n<br>","category":"page"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"import Dates\nstring(\"Last run: \", Dates.unix2datetime(mtime(\"../../../analysis/scaling_multithreading/time_vs_nthreads_0.0001.png\")))","category":"page"},{"location":"devel/analysis/#Matrix-size","page":"Analysis","title":"Matrix size","text":"","category":"section"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"<img src=\"../../assets/imgs/time_vs_size_0.001.png\" width=\"80%\"/>\n<br>","category":"page"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"import Dates\nstring(\"Last run: \", Dates.unix2datetime(mtime(\"../../../analysis/scaling_matrixsize/time_vs_size_0.001.png\")))","category":"page"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"<br>\n<img src=\"../../assets/imgs/time_vs_size_0.0001.png\" width=\"80%\"/>\n<br>","category":"page"},{"location":"devel/analysis/","page":"Analysis","title":"Analysis","text":"import Dates\nstring(\"Last run: \", Dates.unix2datetime(mtime(\"../../../analysis/scaling_matrixsize/time_vs_size_0.0001.png\")))","category":"page"},{"location":"examples/matrix_inv_mpi/","page":"Matrix Inversion (MPI)","title":"Matrix Inversion (MPI)","text":"EditURL = \"https://git.uni-paderborn.de/pc2/julia/submatrixmethod.jl/blob/main/docs/docs/src/examples/matrix_inv_mpi.jl\"","category":"page"},{"location":"examples/matrix_inv_mpi/#Distributed-Matrix-Inversion-(MPI)","page":"Matrix Inversion (MPI)","title":"Distributed Matrix Inversion (MPI)","text":"","category":"section"},{"location":"examples/matrix_inv_mpi/","page":"Matrix Inversion (MPI)","title":"Matrix Inversion (MPI)","text":"using SubmatrixMethod\nusing MPI\nusing LinearAlgebra\nBLAS.set_num_threads(1)\n\nMPI.Init()\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\npinthread(rank) # compact pinning\n\nA = sprandsymposdef(10_000, 0.01);\n\nMPI.Barrier(comm)\nR = submatrix_apply(inv, A, MPISerial())\nif rank == 0 # test on master\n    @show maximum(abs.(R .- R̃))\nend\n\nMPI.Barrier(comm)\nMPI.Finalize()","category":"page"},{"location":"examples/matrix_inv_mpi/","page":"Matrix Inversion (MPI)","title":"Matrix Inversion (MPI)","text":"","category":"page"},{"location":"examples/matrix_inv_mpi/","page":"Matrix Inversion (MPI)","title":"Matrix Inversion (MPI)","text":"This page was generated using Literate.jl.","category":"page"},{"location":"refs/api/#submatrix_api","page":"API","title":"Submatrix Method: API","text":"","category":"section"},{"location":"refs/api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"refs/api/","page":"API","title":"API","text":"Pages   = [\"api.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/api/#References","page":"API","title":"References","text":"","category":"section"},{"location":"refs/api/","page":"API","title":"API","text":"Modules = [SubmatrixMethod]\nPages   = [\"api.jl\"]","category":"page"},{"location":"refs/api/#SubmatrixMethod.HDF5Input","page":"API","title":"SubmatrixMethod.HDF5Input","text":"Represents a matrix in an HDF5 File that has been stored with SubmatrixMethod.write_hdf5.\n\nFields / keyword arguments:\n\nfname: filename / path to file\npath: path to group that contains the matrix inside the file\nmmap (default: false): toggle memory mapping\n\n\n\n\n\n","category":"type"},{"location":"refs/api/#SubmatrixMethod.submatrix_apply","page":"API","title":"SubmatrixMethod.submatrix_apply","text":"submatrix_apply(f, file::HDF5Input, [, lc=Serial()])\n\nReads the input matrix A from the given file::HDF5Input and calls submatrix_apply(f, A, lc) afterwards.\n\nSee HDF5Input.\n\n\n\n\n\n","category":"function"},{"location":"refs/api/#SubmatrixMethod.submatrix_apply-2","page":"API","title":"SubmatrixMethod.submatrix_apply","text":"submatrix_apply(f, A[, lc=Serial()])\n\nUses the submatrix method to approximately compute f(A), i.e. the application of the matrix function f to the input matrix A.\n\nThe input matrix A must be\n\npositive definite (check with isposdef),\nsymmetric (check with issymmetric).\n\nand should be\n\nsparse (check with sparsity or density).\n\nA LaunchConfiguration, such as Serial or Threaded, which specifies the modus operandi can be provided as a third argument (lc).\n\n\n\n\n\n","category":"function"},{"location":"refs/utility/#Utility","page":"Utility","title":"Utility","text":"","category":"section"},{"location":"refs/utility/#Index","page":"Utility","title":"Index","text":"","category":"section"},{"location":"refs/utility/","page":"Utility","title":"Utility","text":"Pages   = [\"utility.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/utility/#References","page":"Utility","title":"References","text":"","category":"section"},{"location":"refs/utility/","page":"Utility","title":"Utility","text":"Modules = [SubmatrixMethod]\nPages   = [\"utility.jl\", \"sprandsymposdef.jl\"]","category":"page"},{"location":"refs/utility/#SubmatrixMethod.MPI_Bcast-Union{Tuple{T}, Tuple{T, Integer, MPI.Comm}} where T","page":"Utility","title":"SubmatrixMethod.MPI_Bcast","text":"Bcast(obj::T, root::Integer, comm::Comm) where T\n\nBroadcast the obj from root to all processes in comm. Returns the object.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.count_nz_in_column-Tuple{SparseArrays.SparseMatrixCSC, Integer}","page":"Utility","title":"SubmatrixMethod.count_nz_in_column","text":"Number of nonzero elements in column j of the sparse matrix S\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.count_nz_per_column-Tuple{SparseArrays.SparseMatrixCSC}","page":"Utility","title":"SubmatrixMethod.count_nz_per_column","text":"Number of nonzero elements per column\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.density-Tuple{AbstractArray{<:Number}}","page":"Utility","title":"SubmatrixMethod.density","text":"Number of non-zero elements over the total number of elements.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.distribute_evenly-Union{Tuple{T}, Tuple{Integer, AbstractVector{T}}} where T<:Integer","page":"Utility","title":"SubmatrixMethod.distribute_evenly","text":"distribute_evenly(nbins::Integer, weights::AbstractVector) -> bins, bin_weights\n\nDistributes the weights as evenly as possible across nbins bins.\n\nReturn values:\n\nbins: a vector of vectors containing the idcs of the entries in weights that have been assigned to each bin.\nbin_weights: the total weights of the bins.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.distribute_naively-Tuple{Integer, Integer}","page":"Utility","title":"SubmatrixMethod.distribute_naively","text":"Naive distribution of ntask many tasks across nbins many bins: splits up work into equal bits and lets the last worker compensates for a potential remainder.\n\nReturn values:\n\nbins: a vector of vectors containing the idcs of the ntask many tasks that have been assigned to each bin.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.find_nonempty_columns-Tuple{SparseArrays.SparseMatrixCSC}","page":"Utility","title":"SubmatrixMethod.find_nonempty_columns","text":"Returns the indices of non-empty columns, i.e. columns with at least one nonzero element.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.inv!-Tuple{Any}","page":"Utility","title":"SubmatrixMethod.inv!","text":"inv!(M)\n\nIn-place matrix inversion. Overwrites the input matrix M with the result.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.similar_zeroed-Union{Tuple{SparseArrays.SparseMatrixCSC{Tv}}, Tuple{Tv}} where Tv","page":"Utility","title":"SubmatrixMethod.similar_zeroed","text":"Creates a similar SparseMatrixCSC, i.e. with the same sparsity pattern, but with all non-zero values in S replaced by zeros.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.sparsity-Tuple{AbstractArray{<:Number}}","page":"Utility","title":"SubmatrixMethod.sparsity","text":"Number of zero-valued elements over the total number of elements.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.spectral_norm-Tuple{AbstractMatrix{<:Number}}","page":"Utility","title":"SubmatrixMethod.spectral_norm","text":"Computes the spectral norm of the input matrix:\n\nA_2=sqrtlambda_max left(A^H Aright)\n\ni.e. the square root of the maximal eigenvalue of A'*A.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.sprandsymposdef-Tuple{Any, Any, Any}","page":"Utility","title":"SubmatrixMethod.sprandsymposdef","text":"sprandsymposdef(n,p,c)\n\nGenerates a sparse matrix of size (n,n) with the following properties:\n\nissymmetric\nisposdef\nhas approximately p * n^2 non-zero elements\nhas a condition number of exactly c\n\nWARNING: While it works okish for some inputs it can take ages to converge (if at all) for others!\n\nAlgorithm:\n\nCreates a diagonal positive-definite matrix with the desired condition number and applies random Jacobi rotations to it to create non-zero off-diagonal elements.\n\nReferences:\n\nThe function is inspired by MATLABs sprandsym(n,density,rc,kind=1).\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#SubmatrixMethod.sprandsymposdef-Tuple{Any, Any}","page":"Utility","title":"SubmatrixMethod.sprandsymposdef","text":"sprandsymposdef(n,p)\n\nGenerates a sparse matrix of size (n,n) with the following properties:\n\nissymmetric\nisposdef\nhas approximately p * n^2 non-zero elements\nhas a condition number of approximately c ≈ 1.\n\nAlgorithm:\n\nCreates a random sparse matrix (sprandn), symmetrizes it and makes it diagonally dominant by adding n*I to ensure positive definiteness.\n\nReferences:\n\nThe implementation of the function is inspired by this stackexchange thread.\n\n\n\n\n\n","category":"method"},{"location":"explanations/smmethod/#submatrixmethod","page":"Submatrix Method","title":"Submatrix Method","text":"","category":"section"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"<figure>\n    <img src=\"../../assets/imgs/smmethod.svg\" width=\"80%\"/>\n    <figcaption>Fig.1 – Illustration of the submatrix method.</figcaption>\n</figure>","category":"page"},{"location":"explanations/smmethod/#Strategy","page":"Submatrix Method","title":"Strategy","text":"","category":"section"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"We consider a sparse, symmetric, n times n, input matrix A for which we want to compute the matrix function f(A). The strategy of the submatrix method is simple: instead of computing f(A) directly, we","category":"page"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"decompose A into n submatrices S_i,\napply f to each submatrix,\nand approximate f(A) from the submatrix results f(S_i) (see Fig. 1).","category":"page"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"Specifically, a submatrix S_i is associated with the i-th column of A and is constructed as follows. First, we find the row indices gamma of all the non-zero elements in column i. Next, we extract those elements from A that correspond to all pairs that we can form from gamma and \"glue them together\" to build the submatrix S_i. Note that while S_i is associated with column i, in particular its non-zero entries, it still contains elements from other columns as well.","category":"page"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"Once we have the submatrices S_i, we compute f(S_i) for each of them. Afterwards, the desired result f(A) is approximated by the result matrix R which has the same sparsity pattern as A - that is the count and position of non-zero entries is preserved - but whose values in a column i are given by the corresponding column tildei of f(S_i) (see Fig. 1). Hence, each submatrix computation produces one column of the result matrix.","category":"page"},{"location":"explanations/smmethod/#Complexity-(Single-Threaded)","page":"Submatrix Method","title":"Complexity (Single-Threaded)","text":"","category":"section"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"(Note that we will neglect the runtime cost of the assembly of the submatrices and the final result which is expected to be irrelevant in the asymptotic limit.)","category":"page"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"Let I(n) be the computational complexity associated with the application of the given matrix function f and let d be the density of the input matrix A. On average, submatrices are then of size m times m where with m = d cdot n. Given that the density d is small enough, one has","category":"page"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"beginaligned\nS(nd) = n cdot I(d cdot n)  I(n)\nendaligned","category":"page"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"in which case the single-threaded submatrtix method computation S(nd) is faster than applying f directly.","category":"page"},{"location":"explanations/smmethod/#Asymptotic-scaling","page":"Submatrix Method","title":"Asymptotic scaling","text":"","category":"section"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"Let's assume that I(n) = Omega(n^p), i.e. that the complexity of computing f(A) is at least n^p. In this case, the relation above simplifies to d  n^-1p. This implies that if the density d decreases at a rate faster than n^-1p than S(nd)  I(n), i.e. the approximate submatrix method computation is computationally cheaper than the regular application of f.","category":"page"},{"location":"explanations/smmethod/#Parallelization","page":"Submatrix Method","title":"Parallelization","text":"","category":"section"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"The crucial feature of the submatrix method is that the individual submatrix computations f(S_i) are entirely independent from each other. This paves the way for massive parallelization where submatrices are processed by different threads (multithreading) or nodes (distributed computing). Data communication is only necessary for distributing the relevant matrix elements at the start of the procedure and collecting the result in the end. Assuming that we can distribute each submatrix computation to a separate node, the overall execution time can be held constant.","category":"page"},{"location":"explanations/smmethod/#References","page":"Submatrix Method","title":"References","text":"","category":"section"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"A Massively Parallel Algorithm for the Approximate Calculation of Inverse p-th Roots of Large Sparse Matrices   Michael Lass, Stephan Mohr, Hendrik Wiebeler, Thomas D. Kühne, Christian Plessl   GitHub repository: https://github.com/pc2/SubmatrixMethod","category":"page"},{"location":"explanations/smmethod/","page":"Submatrix Method","title":"Submatrix Method","text":"A Submatrix-Based Method for Approximate Matrix Function Evaluation in the Quantum Chemistry Code CP2K   Michael Lass, Robert Schade, Thomas D. Kühne, Christian Plessl   arXiv: https://arxiv.org/abs/2004.10811","category":"page"},{"location":"refs/io/#IO","page":"IO","title":"IO","text":"","category":"section"},{"location":"refs/io/","page":"IO","title":"IO","text":"Writing, reading, and memory mapping SparseMatrixCSC to and from disk.","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"using SubmatrixMethod, SparseArrays","category":"page"},{"location":"refs/io/#HDF5","page":"IO","title":"HDF5","text":"","category":"section"},{"location":"refs/io/","page":"IO","title":"IO","text":"Functionality for writing, reading, or memory mapping a SparseMatrixCSC[Tv, Ti} into or from a specific path in a HDF5 file. For memory mapping, the value type (Tv) must be isbitstype (e.g Float64, Float32. or ComplexF64).","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"We'll just write / read the fields of the SparseMatrixCSC into / from separate datasets under the given path.","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"Example:","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"S = sprand(100, 100, 0.001);\nSubmatrixMethod.write_hdf5(\"mymatrix.h5\", \"S\", S)\nisfile(\"mymatrix.h5\")\nT = SubmatrixMethod.read_hdf5(\"mymatrix.h5\", \"S\");\nS == T\nR = SubmatrixMethod.read_hdf5(\"mymatrix.h5\", \"S\"; mmap=true);\nR == T\nrm(\"mymatrix.h5\") # hide","category":"page"},{"location":"refs/io/#Single-binary-file","page":"IO","title":"Single binary file","text":"","category":"section"},{"location":"refs/io/","page":"IO","title":"IO","text":"We define our own Binary Compressed Sparse Column (BCSC) format for storing a SparseMatrixCSC{Tv, Ti} on disk. The value type Tv must be a regular floating point type (e.g. Float64 or Float32) and the index type Ti must be a regular integer type (e.g. Int64 or Int128).","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"\"Definition\" of the format based on the SparseMatrixCSC data structure:","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"# header\nwrite(f, sizeof(Tv))\nwrite(f, sizeof(Ti))\nwrite(f, length(S.colptr))\nwrite(f, length(S.rowval))\nwrite(f, length(S.nzval))\n# data\nwrite(f, S.m)\nwrite(f, S.n)\nwrite(f, S.colptr)\nwrite(f, S.rowval)\nwrite(f, S.nzval)","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"Example:","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"S = sprand(100, 100, 0.001);\nSubmatrixMethod.write_bcsc(\"mymatrix.bcsc\", S)\nisfile(\"mymatrix.bcsc\")\nT = SubmatrixMethod.read_bcsc(\"mymatrix.bcsc\");\nS == T\nR = SubmatrixMethod.read_bcsc(\"mymatrix.bcsc\"; mmap=true);\nR == T\nrm(\"mymatrix.bcsc\") # hide","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"warning: Warning\nAs for now, we don't care about endianness etc. which means that, in general, our format is not portable! As an alternative, consider using HDF5 instead.","category":"page"},{"location":"refs/io/#Three-binary-files","page":"IO","title":"Three binary files","text":"","category":"section"},{"location":"refs/io/","page":"IO","title":"IO","text":"The fields colptr, rowval, and nzval of the SparseMatrixCSC are simply stored in separate binary files with the file extensions .colptr, .rowval, and .nzval, respectively.","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"Example:","category":"page"},{"location":"refs/io/","page":"IO","title":"IO","text":"S = sprand(100, 100, 0.001);\nSubmatrixMethod.write_three_files(\"mymatrix\", S)\nfilter(startswith(\"mymatrix\"), readdir())\nT = SubmatrixMethod.read_three_files(\"mymatrix\");\nS == T\nR = SubmatrixMethod.read_three_files(\"mymatrix\"; mmap=true);\nR == T\nrm.(filter(startswith(\"mymatrix\"), readdir())) # hide","category":"page"},{"location":"refs/io/#Index","page":"IO","title":"Index","text":"","category":"section"},{"location":"refs/io/","page":"IO","title":"IO","text":"Pages   = [\"io.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/io/#References","page":"IO","title":"References","text":"","category":"section"},{"location":"refs/io/","page":"IO","title":"IO","text":"Modules = [SubmatrixMethod]\nPages   = [\"io.jl\"]","category":"page"},{"location":"refs/io/#SubmatrixMethod.read_bcsc-Tuple{AbstractString}","page":"IO","title":"SubmatrixMethod.read_bcsc","text":"Creates a S::SparseMatrixCSC from a binary file in BCSC format, e.g. created using write_bcsc.\n\nKeyword arguments:\n\nmmap (default: false): If true, the data of the arrays colptr, rowval, and nzval within the returned SparseMatrixCSC is linked to the data in the binary file on disk (memory mapping).\n\nSee: write_bcsc\n\n\n\n\n\n","category":"method"},{"location":"refs/io/#SubmatrixMethod.read_hdf5-Tuple{AbstractString, AbstractString}","page":"IO","title":"SubmatrixMethod.read_hdf5","text":"Creates a S::SparseMatrixCSC from the dataset at path in the HDF5 file fname, which should have been created using write_hdf5.\n\nKeyword arguments:\n\nmmap (default: false): If true, the data of the arrays colptr, rowval, and nzval within the returned SparseMatrixCSC is linked to the data in the HDF5 file on disk (memory mapping).\n\nSee: write_hdf5\n\n\n\n\n\n","category":"method"},{"location":"refs/io/#SubmatrixMethod.read_three_files","page":"IO","title":"SubmatrixMethod.read_three_files","text":"Creates a S::SparseMatrixCSC by reading the binary data of the fields from the files prefix.colptr, prefix.rowval, and prefix.nzval.\n\nImportant note: This functions assumes that the sparse matrix is square, i.e. that m == n!\n\nKeyword arguments:\n\nmmap (default: false): If true, the data of the arrays colptr, rowval, and nzval within the returned SparseMatrixCSC is linked to the respective binary files on disk (memory mapping).\n\nSee: write_three_files\n\n\n\n\n\n","category":"function"},{"location":"refs/io/#SubmatrixMethod.read_txt","page":"IO","title":"SubmatrixMethod.read_txt","text":"Read in txt files produced by Matlab (see Michael's original implementation) and return a SparseMatrixCSC.\n\n\n\n\n\n","category":"function"},{"location":"refs/io/#SubmatrixMethod.write_bcsc-Union{Tuple{Ti}, Tuple{Tv}, Tuple{AbstractString, SparseArrays.SparseMatrixCSC{Tv, Ti}}} where {Tv, Ti}","page":"IO","title":"SubmatrixMethod.write_bcsc","text":"Writes a SparseMatrixCSC to file in our custom BCSC (Binary Compressed Sparse Column) format.\n\nSee: read_bcsc\n\n\n\n\n\n","category":"method"},{"location":"refs/io/#SubmatrixMethod.write_hdf5-Tuple{AbstractString, AbstractString, SparseArrays.SparseMatrixCSC}","page":"IO","title":"SubmatrixMethod.write_hdf5","text":"Writes a SparseMatrixCSC into the HDF5 file fname under path. Will create the file if it doesn't exist yet.\n\nSee: read_hdf5\n\n\n\n\n\n","category":"method"},{"location":"refs/io/#SubmatrixMethod.write_three_files-Tuple{AbstractString, SparseArrays.SparseMatrixCSC}","page":"IO","title":"SubmatrixMethod.write_three_files","text":"Writes S::SparseMatrixCSC to three binary files prefix.colptr, prefix.rowval, and prefix.nzval storing the respective fieldvalues of the sparse matrix.\n\nSee: read_three_files\n\n\n\n\n\n","category":"method"},{"location":"refs/lc/#Launch-Configurations","page":"Launch Configurations","title":"Launch Configurations","text":"","category":"section"},{"location":"refs/lc/#Index","page":"Launch Configurations","title":"Index","text":"","category":"section"},{"location":"refs/lc/","page":"Launch Configurations","title":"Launch Configurations","text":"Pages   = [\"lc.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/lc/#References","page":"Launch Configurations","title":"References","text":"","category":"section"},{"location":"refs/lc/","page":"Launch Configurations","title":"Launch Configurations","text":"Modules = [SubmatrixMethod]\nPages   = [\"lc.jl\"]","category":"page"},{"location":"refs/lc/#SubmatrixMethod.DistributedSerial","page":"Launch Configurations","title":"SubmatrixMethod.DistributedSerial","text":"Use multiprocessing via Distributed.jl. On each Julia worker, run things in serial.\n\nFields / keyword arguments:\n\ninplace (default: false): if true, expects that f is an in-place matrix function that overwrites the input matrix with the result, i.e. inv! instead of inv, for example.\nbalance (default: true): if true, we distribute submatrices as evenly as possible (w.r.t. their size) across Julia workers.\nshow_distribution (default: false): if true, we print the distribution of submatrices and their weight (i.e. size) across Julia workers. If UnicodePlots.jl is loaded, bar plots are produced.\n\n\n\n\n\n","category":"type"},{"location":"refs/lc/#SubmatrixMethod.LaunchConfiguration","page":"Launch Configurations","title":"SubmatrixMethod.LaunchConfiguration","text":"An abstract launch configuration for submatrix_apply. See subtypes(LaunchConfiguration) for a list of available launch configurations.\n\n\n\n\n\n","category":"type"},{"location":"refs/lc/#SubmatrixMethod.MPISerial","page":"Launch Configurations","title":"SubmatrixMethod.MPISerial","text":"Use multiprocessing via MPI. On each MPI worker, run things in serial.\n\nWhen using this configuration, the result of submatrix_apply is only returned on the master process whereas nothing is returned on all other MPI processes (workers).\n\nFields / keyword arguments:\n\ninplace (default: false): if true, expects that f is an in-place matrix function that overwrites the input matrix with the result, i.e. inv! instead of inv, for example.\nverbose (default: false): if true, outputs MPI communication meta information etc.\nbalance (default: true): if true, we distribute submatrices as evenly as possible (w.r.t. their size) across MPI workers.\nshow_distribution (default: false): if true, we print the distribution of submatrices and their weight (i.e. size) across MPI workers. If UnicodePlots.jl is loaded, bar plots are produced.\n\n\n\n\n\n","category":"type"},{"location":"refs/lc/#SubmatrixMethod.Serial","page":"Launch Configurations","title":"SubmatrixMethod.Serial","text":"No parallelism. (BLAS might be parallelized, of course.)\n\nFields / keyword arguments:\n\ninplace (default: false): if true, expects that f is an in-place matrix function that overwrites the input matrix with the result, i.e. inv! instead of inv, for example.\n\n\n\n\n\n","category":"type"},{"location":"refs/lc/#SubmatrixMethod.Threaded","page":"Launch Configurations","title":"SubmatrixMethod.Threaded","text":"Multithreading.\n\nFields / keyword arguments:\n\nmode (default: :threads): If :spawn, @spawn is used, i.e. tasks and dynamic load balancing. If :batch, Polyester's @batch is used, i.e. no load balancing. Else, @threads :static is used, i.e. no load balancing.\nnthreads (default: Threads.nthreads()): For some modes, this allows one to set the number of used Julia threads. Threads.nthreads() is the default and an upper bound.\nbalance (default: false): if true, we distribute submatrices as evenly as possible (w.r.t. their size) across threads. Not available for mode = :spawn.\nshow_distribution (default: false): if true, we print the distribution of submatrices and their weight (i.e. size) across threads. If UnicodePlots.jl is loaded, bar plots are produced.\ninplace (default: false): if true, expects that f is an in-place matrix function that overwrites the input matrix with the result, i.e. inv! instead of inv, for example.\n\n\n\n\n\n","category":"type"},{"location":"devel/benchmarking/#Benchmarking","page":"Benchmarking","title":"Benchmarking","text":"","category":"section"},{"location":"devel/benchmarking/#Instrumented-Profiling-([TimerOutputs.jl](https://github.com/KristofferC/TimerOutputs.jl))","page":"Benchmarking","title":"Instrumented Profiling (TimerOutputs.jl)","text":"","category":"section"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"By default, the benchmarking facilities are turned off entirely to avoid any performance overhead.","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"warning: Warning\nThis form of benchmarking doesn't currently work for Threaded()and you'll likely get lot's of errors. See Thread Timers for an alternative.","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"Enable the built-in time measurements with SubmatrixMethod.enable_benchmarks().\nRun the functionality that you want to benchmark.\nCall SubmatrixMethod.print_benchmarks() to see the timing results.","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"using SubmatrixMethod\nSubmatrixMethod.enable_benchmarks()\nA = sprandsymposdef(1000, 0.01);\nsubmatrix_apply(inv, A);\nsubmatrix_apply(inv, A);\nSubmatrixMethod.print_benchmarks()\nSubmatrixMethod.disable_benchmarks() # hide","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"Benchmarks can be reset by calling SubmatrixMethod.reset_benchmarks() or turned off again via SubmatrixMethod.disable_benchmarks().","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"note: Note\nYou can safely call all *_benchmarks functions in combination with @everywhere, i.e. when using multiple Julia workers (Distributed.jl).","category":"page"},{"location":"devel/benchmarking/#Thread-Timers","page":"Benchmarking","title":"Thread Timers","text":"","category":"section"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"As an attempt to assess the load-balancing when using multiple threads, Threaded() has a timers option (default: false) which toggles time measurements on individual threads.","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"using SubmatrixMethod\nusing ThreadPinning; pinthreads(:compact); # hide\nA = sprandsymposdef(1000, 0.1);\nsubmatrix_apply(inv, A, Threaded(timers=false)); # hide\nsubmatrix_apply(inv, A, Threaded(timers=true));","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"If you also load the UnicodePlots.jl package you'll automatically get a nice visual output instead.","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"using UnicodePlots\nusing ThreadPinning; pinthreads(:compact); # hide\nsubmatrix_apply(inv, A, Threaded(timers=true));\nsubmatrix_apply(inv, A, Threaded(timers=true, nthreads=5));","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"Note that UnicodePlots.jl needs to be installed separately and must be loaded after using SubmatrixMethod.","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"You can always (re-)print the timings of the last run via SubmatrixMethod.print_thread_timers(). To access the values themselves, they are stored in SubmatrixMethod.thread_timers[].","category":"page"},{"location":"devel/benchmarking/#Distribution-of-Submatrices-/-Workload","page":"Benchmarking","title":"Distribution of Submatrices / Workload","text":"","category":"section"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"To see how the submatrices have been divided among threads, use the show_distribution option of the Threaded() launch configuration. As for the thread timers above, you can readily get bar plot visualizations just by using UnicodePlots.","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"using SubmatrixMethod, UnicodePlots\nusing ThreadPinning; pinthreads(:compact); # hide\nA = sprandsymposdef(1000, 0.01);\nsubmatrix_apply(inv, A, Threaded(show_distribution=true));","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"We can set balance=true to enable balancing w.r.t. the total weight (sizes) of the submatrices each thread has to handle.","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"submatrix_apply(inv, A, Threaded(balance=true, show_distribution=true));","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"Note that the distribution of the sizes of the submatrices can we visualized with SubmatrixMethod.submatrix_sizes_hist.","category":"page"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"A = sprandsymposdef(10_000, 0.01);\nSubmatrixMethod.submatrix_sizes_hist(A);","category":"page"},{"location":"devel/benchmarking/#References","page":"Benchmarking","title":"References","text":"","category":"section"},{"location":"devel/benchmarking/#Index","page":"Benchmarking","title":"Index","text":"","category":"section"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"Pages   = [\"benchmarking.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"devel/benchmarking/#Functions","page":"Benchmarking","title":"Functions","text":"","category":"section"},{"location":"devel/benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"Modules = [SubmatrixMethod]\nPages   = [\"debugging.jl\"]","category":"page"},{"location":"devel/benchmarking/#SubmatrixMethod.disable_benchmarks-Tuple{}","page":"Benchmarking","title":"SubmatrixMethod.disable_benchmarks","text":"disable_benchmarks()\n\nDisables benchmarking.\n\nSee: enable_benchmarks\n\n\n\n\n\n","category":"method"},{"location":"devel/benchmarking/#SubmatrixMethod.enable_benchmarks-Tuple{}","page":"Benchmarking","title":"SubmatrixMethod.enable_benchmarks","text":"enable_benchmarks()\n\nEnables benchmarking. This affects all SubmatrixMethod.@bench macro applications.\n\nResults can be printed via SubmatrixMethod.print_benchmarks() and reset via SubmatrixMethod.reset_benchmarks().\n\nSee: disable_benchmarks\n\n\n\n\n\n","category":"method"},{"location":"devel/benchmarking/#SubmatrixMethod.print_benchmarks-Tuple{}","page":"Benchmarking","title":"SubmatrixMethod.print_benchmarks","text":"Print benchmark results.\n\nSee: enable_benchmarks\n\n\n\n\n\n","category":"method"},{"location":"devel/benchmarking/#SubmatrixMethod.print_submatrix_distribution-Tuple{Any, Any}","page":"Benchmarking","title":"SubmatrixMethod.print_submatrix_distribution","text":"Print the distribution of submatrices and their weights.\n\nIf UnicodePlots.jl is loaded (after using SubmatrixMethod), this function will produce a bar plot.\n\n\n\n\n\n","category":"method"},{"location":"devel/benchmarking/#SubmatrixMethod.print_thread_timers-Tuple{}","page":"Benchmarking","title":"SubmatrixMethod.print_thread_timers","text":"Print the current state of the thread timers.\n\nIf UnicodePlots.jl is loaded (after using SubmatrixMethod), this function will produce a bar plot.\n\n\n\n\n\n","category":"method"},{"location":"devel/benchmarking/#SubmatrixMethod.print_timings-Tuple{AbstractVector{<:Real}}","page":"Benchmarking","title":"SubmatrixMethod.print_timings","text":"Pretty print the given timings.\n\nIf UnicodePlots.jl is loaded (after using SubmatrixMethod), this function will produce a bar plot.\n\n\n\n\n\n","category":"method"},{"location":"devel/benchmarking/#SubmatrixMethod.reset_benchmarks-Tuple{}","page":"Benchmarking","title":"SubmatrixMethod.reset_benchmarks","text":"Reset benchmark results.\n\n\n\n\n\n","category":"method"},{"location":"devel/benchmarking/#SubmatrixMethod.reset_thread_timers-Tuple{}","page":"Benchmarking","title":"SubmatrixMethod.reset_thread_timers","text":"Reset the thread timers to zero.\n\n\n\n\n\n","category":"method"},{"location":"devel/benchmarking/#SubmatrixMethod.@bench-Tuple{Any, Any}","page":"Benchmarking","title":"SubmatrixMethod.@bench","text":"Usage: @bench \"some description\" 3+3\n\nSee: enable_benchmarks\n\n\n\n\n\n","category":"macro"},{"location":"devel/guide/#Contribution-Guide","page":"Contribution Guide","title":"Contribution Guide","text":"","category":"section"},{"location":"devel/guide/#Conventions","page":"Contribution Guide","title":"Conventions","text":"","category":"section"},{"location":"devel/guide/#Source-code","page":"Contribution Guide","title":"Source code","text":"","category":"section"},{"location":"devel/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"As indicated by the .JuliaFormatter.toml, we follow the BlueStyle for code formatting.\nEasiest way to make sure that code is formatted correctly is to use the autoformatting feature of the Julia VSCode extension.\nAlternatively, you can use JuliaFormatter.jl directly.\nThe user-provided input matrix is always labeled A.\nThe result matrix is always labeled R.","category":"page"},{"location":"devel/guide/#Documentation","page":"Contribution Guide","title":"Documentation","text":"","category":"section"},{"location":"devel/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"Philosophically, we loosly try to implement the ideas put forward by David Laing in the documentation system a.k.a \"The Grand Unified Theory of Documentation\". TLDR:","category":"page"},{"location":"devel/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"\"References\" should only contain technical descriptions (if examples, only very minimal ones)\n\"Explanations\" contains methods discussions / descriptions with formulas etc.\n\"Examples\" is for How-To guides\n(We don't really have Tutorials I guess?)","category":"page"},{"location":"devel/guide/#Structure-of-the-code","page":"Contribution Guide","title":"Structure of the code","text":"","category":"section"},{"location":"devel/guide/#Adding-a-new-LaunchConfiguration","page":"Contribution Guide","title":"Adding a new LaunchConfiguration","text":"","category":"section"},{"location":"devel/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"Say we wanted to add a new launch configuration UltraParallel :)","category":"page"},{"location":"devel/guide/","page":"Contribution Guide","title":"Contribution Guide","text":"Create UltraParallel in lc.jl.\nCreate a file, say, ultraparallel.jl in src/.\nIn this file, create special methods for the generic functions defined in api.jl (as desired)\nsubmatrix_apply(f, A::AbstractMatrix, lc::UltraParallel)\nsubmatrix_apply(f, file::HDF5Input, lc::UltraParallel)\nTypically, you want to use these high-level methods to perform some checks on the input arguments, process the keyword arguments of UltraParallel etc. and then call, say, _ultraparallel(...) which actually sets up the parallelization.\nThe actual computation kernel should go into a separate function, say, _ultraparallel_kernel(...).\nNote that you can use the core submatrix functionality defined in submatrix_core.jl (see Submatrix Method: Core) in this kernel, so that you don't have to reinvent the core part of the computation. I.e. you typically would want to reuse (or extend if necessary)\nsubmatrix_computation!\nsubmatrix_computation_inplace!","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"EditURL = \"https://git.uni-paderborn.de/pc2/julia/submatrixmethod.jl/blob/main/docs/docs/src/examples/matrix_inv_distributed.jl\"","category":"page"},{"location":"examples/matrix_inv_distributed/#Distributed-Matrix-Inversion","page":"Matrix Inversion (Distributed)","title":"Distributed Matrix Inversion","text":"","category":"section"},{"location":"examples/matrix_inv_distributed/#No-parallelism","page":"Matrix Inversion (Distributed)","title":"No parallelism","text":"","category":"section"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"Preparation.","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"using SubmatrixMethod, MKL, LinearAlgebra, BenchmarkTools\nBLAS.set_num_threads(1)","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"We need a random input matrix.","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"A = sprandsymposdef(1000, 0.001)","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"We can get a (good) approximation of the inverse of A via submatrix_apply(inv, A).","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"Ainv = submatrix_apply(inv, A)\nmaximum(abs.(Ainv .- inv(Matrix(A))))","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"Let's benchmark the serial submatrix method","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"@btime submatrix_apply($inv, $A);\nnothing #hide","category":"page"},{"location":"examples/matrix_inv_distributed/#DistributedSerial","page":"Matrix Inversion (Distributed)","title":"DistributedSerial","text":"","category":"section"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"Add a few Julia workers.","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"using Distributed\nwithenv(\"JULIA_PROJECT\" => @__DIR__) do\n    addprocs(5)\nend","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"Prepare all of them.","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"@everywhere begin\n    using SubmatrixMethod\n    using MKL\n    using LinearAlgebra\n    BLAS.set_num_threads(1)\nend","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"Let's see if things work correctly.","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"Ainv = submatrix_apply(inv, A, DistributedSerial())\nmaximum(abs.(Ainv .- inv(Matrix(A))))","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"And now let's benchmark.","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"@btime submatrix_apply($inv, $A, $(DistributedSerial()));","category":"page"},{"location":"examples/matrix_inv_distributed/#HDF5Input","page":"Matrix Inversion (Distributed)","title":"HDF5Input","text":"","category":"section"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"You can also use HDF5Input to make submatrix_apply use a sparse matrix stored in a HDF5 file (should have been stored with write_hdf5). In this case, the input matrix isn't send to workers but instead read / mmapped from disk on every worker. This lowers the inter-worker communication at the cost of (parallel) disk accesses.","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"# if an A::SparseMatrixCSC is provided to the HDF5Input constructor\n# the matrix is written to file before the HDF5Input wrapper is created\nAh5 = HDF5Input(A; fname=\"A.h5\", path=\"A\", mmap=mmap, overwrite=true)\nR = submatrix_apply(inv, Ah5, DistributedSerial())","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"That's it.","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"nworkers() > 1 && rmprocs(workers()) # hide","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"","category":"page"},{"location":"examples/matrix_inv_distributed/","page":"Matrix Inversion (Distributed)","title":"Matrix Inversion (Distributed)","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#SubmatrixMethod.jl","page":"SubmatrixMethod","title":"SubmatrixMethod.jl","text":"","category":"section"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"Approximately compute matrix functions of matrices by using the submatrix method.","category":"page"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"The input matrix must be","category":"page"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"positive definite (check with isposdef),\nsymmetric (check with issymmetric).","category":"page"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"and should be","category":"page"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"sparse (check with sparsity or density).","category":"page"},{"location":"#Installation","page":"SubmatrixMethod","title":"Installation","text":"","category":"section"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"You can simply add SubmatrixMethod.jl to your Julia environment with the command","category":"page"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"] add https://git.uni-paderborn.de/pc2/julia/submatrixmethod.jl","category":"page"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"note: Note\nThe minimal required Julia version is 1.6 but we recommend using Julia ≥ 1.7.","category":"page"},{"location":"#TLDR","page":"SubmatrixMethod","title":"TLDR","text":"","category":"section"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"using SubmatrixMethod\n\n# sparse, symmetric, random, positive definite matrix of\n# size 1000 x 1000 with density around 0.05.\nA = sprandsymposdef(1000, 0.05)\n\nX = inv(Matrix(A))\nY = submatrix_apply(inv, A)\n\nmaximum(abs.(X .- Y) ≤ 1e-7 # true","category":"page"},{"location":"#Acknowledgements","page":"SubmatrixMethod","title":"Acknowledgements","text":"","category":"section"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"SubmatrixMethod.jl is an effort by the Paderborn Center for Parallel Computing (PC²) within the german, national HPC initiative NHR.","category":"page"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"Developers:","category":"page"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"Carsten Bauer (@carstenbauer) (lead developer)","category":"page"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"Other Contributors:","category":"page"},{"location":"","page":"SubmatrixMethod","title":"SubmatrixMethod","text":"Michael Lass (@michaellass)","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"EditURL = \"https://git.uni-paderborn.de/pc2/julia/submatrixmethod.jl/blob/main/docs/docs/src/examples/matrix_inv.jl\"","category":"page"},{"location":"examples/matrix_inv/#Speeding-up-Matrix-Inversion","page":"Matrix Inversion","title":"Speeding up Matrix Inversion","text":"","category":"section"},{"location":"examples/matrix_inv/#Regular-inversion","page":"Matrix Inversion","title":"Regular inversion","text":"","category":"section"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"First, we need to generate a random input matrix M that we want to invert. However, since the submatrix method expects a sparse symmetric positive definite matrix, we can't just use rand. Instead, we use the utility function sprandsymposdef to generate our input matrix.","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"using SubmatrixMethod\nSubmatrixMethod.disable_benchmarks() # hide\nusing MKL, LinearAlgebra # hide\nBLAS.set_num_threads(1) # hide\nM = sprandsymposdef(1000, 0.001)","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"Note that M isn't just sparse in the value sense but actually a SparseMatrixCSC datastructure.","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"typeof(M)","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"If we try to naively invert this matrix, i.e. calling inv(M), Julia will throw an error, reminding us of the fact that the inverse of a sparse matrix is generally dense. To circumvent this error, we first need to convert M to a dense Matrix{Float64} first.","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"Mdense = Matrix(M);\nnothing #hide","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"Now, inv(M) does what it's supposed to do.","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"using LinearAlgebra, Test\n@test inv(Mdense) * Mdense ≈ I","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"Alright, let's load BenchmarkTools.jl and benchmark how long the conventional inversion takes. This will serve as a baseline that we can compare to the submatrix method.","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"using BenchmarkTools\n@btime inv($Mdense);\nnothing #hide","category":"page"},{"location":"examples/matrix_inv/#Submatrix-method","page":"Matrix Inversion","title":"Submatrix method","text":"","category":"section"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"Using the function submatrix_apply by SubmatrixMethod.jl, we can get a (good) approximation of inv(Mdense) via submatrix_apply(inv, Mdense).","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"M̃inv = submatrix_apply(inv, Mdense)\nMinv = inv(Mdense)\nmaximum(abs.(M̃inv .- Minv))","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"However, note that the computation based on the submatrix method is much faster","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"@btime submatrix_apply($inv, $Mdense);\nnothing #hide","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"And it is even faster if we provide the sparse matrix (SparseMatrixCSC) directly","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"@btime submatrix_apply($inv, $M);\nnothing #hide","category":"page"},{"location":"examples/matrix_inv/#Multithreading","page":"Matrix Inversion","title":"Multithreading","text":"","category":"section"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"Depending on the size/sparsity of the input matrix (see Scaling), we can sometimes speed things up even further by enabling the multithreading functionality of SubmatrixMethod.jl. Of course, this only works if we've started Julia with multiple threads in the first place.","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"Threads.nthreads()","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"To avoid conflicts with BLAS's built-in multithreading, it is generally recommended to set the number of BLAS threads to one.","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"BLAS.set_num_threads(1)","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"Also, it is generally a good idea to pin Julia threads to different cores (preferrably within a NUMA domain). Here, we use the package ThreadPinning.jl to implement a compact pinning strategy.","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"using ThreadPinning\npinthreads(:compact)","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"Alright, here comes a benchmark that shows a case where multithreading gives a decent speedup.","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"M = sprandsymposdef(1000, 0.01)\n@btime submatrix_apply($inv, $M, $(Serial()));\n@btime submatrix_apply($inv, $M, $(Threaded()));\nnothing #hide","category":"page"},{"location":"examples/matrix_inv/#Scaling","page":"Matrix Inversion","title":"Scaling","text":"","category":"section"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"As a function of the density of the input matrix (and for various sizes)\nAs a function of the size of the input matrix (and for various densities)\nAs a function of the number of Julia / BLAS threads","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"So much about speeding up matrix inversion with the submatrix method. See you in the next tutorial!","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"","category":"page"},{"location":"examples/matrix_inv/","page":"Matrix Inversion","title":"Matrix Inversion","text":"This page was generated using Literate.jl.","category":"page"},{"location":"refs/submatrix_core/#submatrix_core","page":"Core","title":"Submatrix Method: Core","text":"","category":"section"},{"location":"refs/submatrix_core/#Index","page":"Core","title":"Index","text":"","category":"section"},{"location":"refs/submatrix_core/","page":"Core","title":"Core","text":"Pages   = [\"submatrix_core.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/submatrix_core/#References","page":"Core","title":"References","text":"","category":"section"},{"location":"refs/submatrix_core/","page":"Core","title":"Core","text":"Modules = [SubmatrixMethod]\nPages   = [\"submatrix_core.jl\"]","category":"page"},{"location":"refs/submatrix_core/#SubmatrixMethod.construct_submatrix-Tuple{AbstractMatrix, Any}","page":"Core","title":"SubmatrixMethod.construct_submatrix","text":"construct_submatrix(A, j) -> submatrix, indices\n\nConstruct the j-th submatrix form the input matrix A.\n\n\n\n\n\n","category":"method"},{"location":"refs/submatrix_core/#SubmatrixMethod.submatrix_computation!-Tuple{Any, Any, AbstractMatrix, Any}","page":"Core","title":"SubmatrixMethod.submatrix_computation!","text":"submatrix_computation!(R, f, A, j)\n\nThis is the \"computation kernel\". Constructs the j-th submatrix from the input matrix A, applies the matrix function f to it, and fills the corresponding column of the result matrix R.\n\n\n\n\n\n","category":"method"},{"location":"refs/submatrix_core/#SubmatrixMethod.submatrix_computation_inplace!-Tuple{Any, Any, AbstractMatrix, Any}","page":"Core","title":"SubmatrixMethod.submatrix_computation_inplace!","text":"submatrix_computation_inplace!(R, f_inplace, A, j)\n\nThis is the \"computation kernel\". Constructs the j-th submatrix from the input matrix A, applies the matrix function f_inplace to it (in-place), and fills the corresponding column of the result matrix R.\n\n\n\n\n\n","category":"method"}]
}
